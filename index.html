<!DOCTYPE html>
<html lang="en">
    <head>

	    
        <title>Swasti Shreya Mishra</title>
       <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="http://swastishreya.me" />
	    <meta property="og:title" content="Swasti Mishra" />
	    <meta property="og:image" content="http://swastishreya.me/img/swasti_mishra.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge"> 
	    <meta name="author" content="Swasti Shreya Mishra">
         <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <!-- <link rel="shortcut icon" type="image/png" href="favicon.ico"/>   style="text-align:center"  -->

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
       
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'> 
	<link rel="stylesheet" href="css/style.css">
    </head>
    <body>
        <div class="container mt-5">
            <div class="row mb-3">
                <div class="col">
                    <h1>Swasti Shreya Mishra</h1>
                </div>
            </div>
            <div class="row">
                <div style="text-align:center" class="col-lg-4 col-md-6 order-0 order-xs-0 order-sm-0 order-md-1 order-lg-1" >
                    <div class="card mb-3">
                        <img class="card-img-top" src="img/swasti_mishra.jpg" alt="Swasti Shreya Mishra">
                        <div class="card-body">
                            <h5 class="card-title">
                                <b>Swasti Shreya Mishra</b>
                            </h5>
                                

				
                            <p class="card-text">
                                Research Associate, Adobe Research (Jul 2022 - Present)
                                </br>
				Integrated Master of Technology, IIIT Bangalore (2017-2022)
<!--                                 <a href="https://www.ee.iitb.ac.in/web"
                        target="_blank">Electrical Engineering</a> (BTech)
                                </br>
			        <a href="https://www.minds.iitb.ac.in/"
                        target="_blank">AI and Data Science</a> (MTech)
                                </br>
                                Indian Institute of Technology Bombay
                                </br> -->
                            </p>
                        </div>
                    </div>
                </div>
                <div class="col-lg-8 col-md-6 order-1 order-xs-1 order-sm-1 order-md-0 order-lg-0">
                    <p>
                        Hi! I'm a Research Associate in the Collaborative Creativity team of <a href="https://research.adobe.com/" target="_blank">Adobe Research</a>, India. My areas of interest include video generation, 3D scene understanding and human-centred AI.
                    </p>
                  
		    <p>    
                            I graduated from <a href="https://www.iiitb.ac.in/" target="_blank">IIIT Bangalore</a>, India, in 2022 with a Bachelor’s degree in Computer Science Engineering and a Master’s specialization in Artificial Intelligence and Machine Learning. 
			    My master's thesis was on DeepFake-based controlled video generation to investigate the role of specific nonverbal cues in job interviews, under the guidance of <a href="https://www.iiitb.ac.in/faculty/dinesh-babu-jayagopi"target="_blank">Prof. Dr. Dinesh Babu Jayagopi</a> (Multimodal Perception Lab), in collaboration with the psychology department of the University of Lausanne. 
			    <!-- During my undergraduate study, I spent six amazing months as a research intern at <a href="https://research.adobe.com/" target="_blank">Adobe Research</a>, India 
			    where I worked with <a href="https://research.adobe.com/person/balaji-vasan-srinivasan/" target="_blank">Dr. Balaji Vasan Srinivasan</a> on problems 
			    related to context-aware scene enrichment, and multimodal question answering.                                                   -->
                    </p>


			
<!--                   <p>
 
                        Hi! I'm a final year IDDDP student pursuing Bachelors in Electrical Engineering and Masters in AI & Data Science at
                        IIT Bombay, where I work in
                        the <a href="http://www.ee.iitb.ac.in/~viplab/"
                        target="_blank"> Vision & Image Processing group</a>. My advisor is Prof. <a href="https://biplab-banerjee.github.io/index.html"
                        target="_blank"> Biplab Banerjee</a>.
			  
                        My research interests broadly revolve around natural language processing and computer vision wherein I'm interested in exploring techniques
                        to bridge the gap between text understanding and image understanding. My current thesis focuses on understanding the
                        generalizability and robustness of deep learning techniques while adapting to new data deficient tasks- commonly known as <i>few-shot class incremental 
			  learning problem</i>.
                    </p> 
                 <p>
                        Previously, I spent six amazing months as a research intern at <a href="https://research.adobe.com/" target="_blank">Adobe Research</a>, India 
			where I worked with <a href="https://research.adobe.com/person/balaji-vasan-srinivasan/" target="_blank">Dr. Balaji Vasan Srinivasan</a>.
                        I've also spent time at the <a href="https://www.ee.iitb.ac.in/web/research/labs/medal" target="_blank">MeDAL Lab</a>, IIT Bombay
			 working with <a href="https://www.ee.iitb.ac.in/web/people/faculty/home/asethi" target="_blank">Dr. Amit Sethi</a>.
                    </p> -->
                     <!--   <p>
                        I’m happy to have discussions with people interested in natural language
                        processing or computer vision—please feel free to email me!
                    </p> -->
                     <br/> <br/> <br/>
<!-- 		  <div class="row">
	                <div class="col"> -->
	                    <p>
	                        Email: mishra.swastishreya13@gmail.com
	                    </p>
	                    <p>
	                        Links:
	                        [<a href="files/Swasti_CV_26_Oct.pdf" target="_blank">CV</a>] [<a href="https://www.linkedin.com/in/swastishreyamishra/" target="_blank">LinkedIn</a>] 
	                    <!--[<a href="https://github.com/nelson-liu" target="_blank">Github</a>] [<a href="https://scholar.google.com/citations?user=ghGDz7MAAAAJ&hl=en" target="_blank">Google Scholar</a>] [<a href="https://blog.nelsonliu.me/" target="_blank">Blog</a>] -->
	                    </p>
<!-- 	                </div>
	            </div> -->
                </div>

		    
		    
            </div>
            
           


               <!-- <div class="row">
                <div class="col">
                    <h2>Recent News</h2>
                    <ul>
                        <li>
                            (8/2019) Paper on Quoref, a new reading
                            comprehension dataset with questions that require
                            resolving coreference, accepted to EMNLP 2019.
                        </li>
                        <li>
                            (8/2019) I'm participating in
                            the <a href="https://cbmm.mit.edu/"
                            target="_blank">Center for Brains, Minds, and
                            Machines'</a> <a href="https://cbmm.mit.edu/summer-school/2019"
                            target="_blank">summer course</a>.
                        </li>
                    </ul>
                </div>
            </div>
            <hr> -->
            <div class="row" id="publications">
                <div class="col">
                    <h2>Publications</h2>

                    <h3>2023</h3>
                    <ul class="pl">
                        
                        <li>
			    <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3590980" target="_blank">
                                <b>SketchBuddy: Context-Aware Sketch Enrichment and Enhancement</b>
                            </a>
                            <br/>

<!-- 			    href="https://2023.acmmmsys.org/" target="_blank" -->
                            In <a>
                                <b>
                                    ACM Multimedia Systems Conference (ACM MMSys)</b></a>, 2023.
                            <br/>

			</li>
                        
		    </ul>
		    <h3>2022</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://doi.org/10.1145/3571600.3571633" target="_blank">
                                <b>A Hybrid Rigid and Non-Rigid Motion Approximation for Generating Realistic Listening Behavior Videos</b>
                            </a>
                            <br/>

<!-- 			    href="https://2022.acmmm.org/" target="_blank" -->
                            In <a>
                                <b>
                                    Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP)</b></a>, 2022.
                            <br/>
			</li>
		    </ul>
                    <h3>2021</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://doi.org/10.1016/j.physa.2021.126298" target="_blank">
                                <b>Firm dynamics and employee performance management in duopoly markets</b>
                            </a>
                            <br/>

<!-- 		            href="https://2021.naacl.org/" target="_blank" -->
                            In <a>
                                <b>
                                    Physica A: Statistical Mechanics and its Applications</b></a>, 2021.
                            <br/>
			</li>
		    </ul>
			
                           <!--  [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#dasigi_liu_marasovic_smith_gardner_emnlp2019_abstract').toggle();return false;">abstract</a>]
                            [<a href="papers/dasigi+liu+marasovic+smith+gardner.emnlp2019.poster.pdf" target="_blank">poster</a>]
                            [<a href="https://quoref-dataset.s3-us-west-2.amazonaws.com/train_and_dev/quoref-train-dev-v0.1.zip" target="_blank">dataset</a>]
                            [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>]
                            <div id="dasigi_liu_marasovic_smith_gardner_emnlp2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Machine comprehension of texts longer than a
                                    single sentence often requires coreference
                                    resolution. However, most current reading
                                    comprehension benchmarks do not contain
                                    complex coreferential phenomena and hence
                                    fail to evaluate the ability of models to
                                    resolve coreference. We present a new
                                    crowdsourced dataset containing more than
                                    24K span-selection questions that require
                                    resolving coreference among entities in
                                    over 4.7K English paragraphs from Wikipedia.
                                    Obtaining questions focused on such
                                    phenomena is challenging, because it is hard
                                    to avoid lexical cues that shortcut
                                    complex reasoning. We deal with this issue
                                    by using a strong baseline model as an
                                    adversary in the crowdsourcing loop, which
                                    helps crowdworkers avoid writing questions
                                    with exploitable surface cues. We show that
                                    state-of-the-art reading comprehension
                                    models perform significantly worse than
                                    humans on this benchmark—the best model
                                    performance is 70.5 F1, while the estimated
                                    human performance is 93.4 F1.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/logan+liu+peters+gardner+singh.acl2019.pdf" target="_blank">
                                <b>Barack's Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling</b>
                            </a>
                            <br/>
                            <a href="https://rloganiv.github.io/" target="_blank">Robert L. Logan IV</a>,
                            <b>Nelson F. Liu</b>,
                            <a href="http://matt-peters.github.io/" target="_blank">Matthew E. Peters</a>,
                            <a href="https://matt-gardner.github.io/" target="_blank">Matt Gardner</a>,
                            and <a href="http://sameersingh.org/" target="_blank">Sameer Singh</a>.
                            <br/>
                            In <a href="http://www.acl2019.org/" target="_blank">
                                <b>Annual Meeting of the Association for Computational Linguistics (ACL)</b></a>, 2019.
                            <br/>
                            [<a href="papers/logan+liu+peters+gardner+singh.acl2019.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#logan_liu_peters_gardner_singh_acl2019_abstract').toggle();return false;">abstract</a>]
                            [<a href="papers/logan+liu+peters+gardner+singh.acl2019.poster.pdf" target="_blank">poster</a>]
                            [<a href="https://github.com/rloganiv/kglm-model" target="_blank">code</a>]
                            [<a href="https://rloganiv.github.io/linked-wikitext-2/" target="_blank">dataset</a>]
                            <div id="logan_liu_peters_gardner_singh_acl2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Modeling human language requires the ability
                                    to not only generate fluent text but also
                                    encode factual knowledge. However,
                                    traditional language models are only capable
                                    of remembering facts seen at training time,
                                    and often have difficulty recalling them. To
                                    address this, we introduce the knowledge
                                    graph language model (KGLM), a neural
                                    language model with mechanisms for selecting
                                    and copying facts from a knowledge graph
                                    that are relevant to the context. These
                                    mechanisms enable the model to render
                                    information it has never seen before, as
                                    well as generate out-of-vocabulary tokens.
                                    We also introduce the Linked WikiText-2
                                    dataset, a corpus of annotated text aligned
                                    to the Wikidata knowledge graph whose
                                    contents (roughly) match the popular
                                    WikiText-2 benchmark (Merity et al., 2017).
                                    In experiments, we demonstrate that the KGLM
                                    achieves significantly better performance
                                    than a strong baseline language model. We
                                    additionally compare different language
                                    models’ ability to complete sentences
                                    requiring factual knowledge, and show that
                                    the KGLM outperforms even very large
                                    language models in generating facts.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.pdf" target="_blank">
                                <b>Linguistic Knowledge and Transferability of Contextual Representations</b>
                            </a>
                            <br/>
                            <b>Nelson F. Liu</b>,
                            <a href="https://matt-gardner.github.io/" target="_blank">Matt Gardner</a>,
                            <a href="http://people.csail.mit.edu/belinkov/" target="_blank">Yonatan Belinkov</a>,
                            <a href="http://matt-peters.github.io/" target="_blank">Matthew E. Peters</a>,
                            and <a href="https://homes.cs.washington.edu/~nasmith/" target="_blank">Noah A. Smith</a>.
                            <br/>
                            In <a href="https://naacl2019.org/" target="_blank">
                                <b>North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2019.
                            <br/>
                            [<a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#liu_gardner_belinkov_peters_smith_naacl2019_abstract').toggle();return false;">abstract</a>]
                            [slides:
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.pdf" target="_blank">pdf</a>,
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.with_speaker_notes.pdf" target="_blank">pdf with notes</a>,
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.key" target="_blank">key</a>]
                            [<a href="papers/contextual-repr-analysis" target="_blank">code</a>]
                            <div id="liu_gardner_belinkov_peters_smith_naacl2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Contextual word representations derived from
                                    large-scale neural language models are
                                    successful across a diverse set of NLP
                                    tasks, suggesting that they encode useful
                                    and transferable features of language. To
                                    shed light on the linguistic knowledge they
                                    capture, we study the representations
                                    produced by several recent pretrained
                                    contextualizers (variants of ELMo, the
                                    OpenAI transformer LM, and BERT) with a
                                    suite of seventeen diverse probing tasks. We
                                    find that linear models trained on top of
                                    frozen contextual representations are
                                    competitive with state-of-the-art
                                    task-specific models in many cases, but fail
                                    on tasks requiring fine-grained linguistic
                                    knowledge (e.g., conjunct identification).
                                    To investigate the transferability of
                                    contextual word representations, we quantify
                                    differences in the transferability of
                                    individual layers within contextualizers,
                                    especially between RNNs and transformers.
                                    For instance, higher layers of RNNs are more
                                    task-specific, while transformer layers do
                                    not exhibit the same monotonic trend. In
                                    addition, to better understand what makes
                                    contextual word representations
                                    transferable, we compare language model
                                    pretraining with eleven supervised
                                    pretraining tasks. For any given task,
                                    pretraining on a closely related task yields
                                    better performance than language model
                                    pretraining (which is better on average)
                                    when the pretraining dataset is fixed.
                                    However, language model pretraining on more
                                    data gives the best results.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/liu+schwartz+smith.naacl2019.pdf" target="_blank">
                                <b>Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets</b>
                            </a>
                            <br/>
                            <b>Nelson F. Liu</b>,
                            <a href="https://homes.cs.washington.edu/~roysch/" target="_blank">Roy Schwartz</a>,
                            and <a href="https://homes.cs.washington.edu/~nasmith/" target="_blank">Noah A. Smith</a>.
                            <br/>
                            In <a href="https://naacl2019.org/" target="_blank">
                                <b>North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2019.
                            <br/>
                            [<a href="papers/liu+schwartz+smith.naacl2019.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#liu_schwartz_smith_naacl2019_abstract').toggle();return false;">abstract</a>]
                            [slides:
                            <a href="papers/liu+schwartz+smith.naacl2019.slides.pdf" target="_blank">pdf</a>,
                            <a href="papers/liu+schwartz+smith.naacl2019.slides.with_speaker_notes.pdf" target="_blank">pdf with notes</a>,
                            <a href="papers/liu+schwartz+smith.naacl2019.slides.key" target="_blank">key</a>]
                            [<a href="papers/inoculation-by-finetuning" target="_blank">code</a>]
                            <div id="liu_schwartz_smith_naacl2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Several datasets have recently been
                                    constructed to expose brittleness in models
                                    trained on existing benchmarks. While model
                                    performance on these challenge datasets is
                                    significantly lower compared to the original
                                    benchmark, it is unclear what particular
                                    weaknesses they reveal. For example, a
                                    challenge dataset may be difficult because
                                    it targets phenomena that current models
                                    cannot capture, or because it simply
                                    exploits blind spots in a model's specific
                                    training set. We introduce inoculation by
                                    fine-tuning, a new analysis method for
                                    studying challenge datasets by exposing
                                    models (the metaphorical patient) to a small
                                    amount of data from the challenge dataset (a
                                    metaphorical pathogen) and assessing how
                                    well they can adapt. We apply our method to
                                    analyze the NLI "stress tests" (Naik et al.,
                                    2018) and the Adversarial SQuAD dataset (Jia
                                    and Liang, 2017). We show that after slight
                                    exposure, some of these datasets are no
                                    longer challenging, while others remain
                                    difficult. Our results indicate that
                                    failures on challenge datasets may lead to
                                    very different conclusions about models,
                                    training datasets, and the challenge
                                    datasets themselves.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2018</h3>
                    <ul class="pl">
                        <li>
                            <a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.pdf" target="_blank">
                                <b>LSTMs Exploit Linguistic Attributes of Data</b>
                            </a>
                            <br/>
                            <b>Nelson F. Liu</b>,
                            <a href="https://levyomer.wordpress.com/" target="_blank">Omer Levy</a>,
                            <a href="https://homes.cs.washington.edu/~roysch/" target="_blank">Roy Schwartz</a>,
                            <a href="https://chenhaot.com/" target="_blank">Chenhao Tan</a>,
                            and <a href="https://homes.cs.washington.edu/~nasmith/" target="_blank">Noah A. Smith</a>.
                            <br/>
                            In <a href="https://sites.google.com/site/repl4nlp2018" target="_blank">
                                <b>ACL Workshop on Representation Learning for NLP (RepL4NLP)</b></a>, 2018.
                            <b><a href="https://sites.google.com/site/repl4nlp2018/accepted-papers" target="_blank" style="color:#e22222">(Best Paper Award)</a></b>.
                            <br/>
                            [<a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#liu_levy_schwartz_tan_smith_repl4nlp2018_abstract').toggle();return false;">abstract</a>]
                            [<a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.slides.pdf" target="_blank">(short) slides</a>]
                            [<a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.poster.pdf" target="_blank">poster</a>]
                            [<a href="papers/lstms-exploit-linguistic-attributes" target="_blank">code</a>]
                            <div id="liu_levy_schwartz_tan_smith_repl4nlp2018_abstract" class="abstract" style="display:none;">
                                <p>
                                    While recurrent neural networks have found
                                    success in a variety of natural language
                                    processing applications, they are general
                                    models of sequential data. We investigate
                                    how the properties of natural language data
                                    affect an LSTM's ability to learn a
                                    nonlinguistic task: recalling elements from
                                    its input. We find that models trained on
                                    natural language data are able to recall
                                    tokens from much longer sequences than
                                    models trained on non-language sequential
                                    data. Furthermore, we show that the LSTM
                                    learns to solve the memorization task by
                                    explicitly using a subset of its neurons to
                                    count timesteps in the input. We hypothesize
                                    that the patterns and structure in natural
                                    language data enable LSTMs to learn by
                                    providing approximate ways of reducing loss,
                                    but understanding the effect of different
                                    training data on the learnability of LSTMs
                                    remains an open question.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/allennlp.nlposs2018.pdf" target="_blank">
                                <b>AllenNLP: A Deep Semantic Natural Language Processing Platform</b>
                            </a>
                            <br/>
                            <a href="https://matt-gardner.github.io/" target="_blank">Matt Gardner</a>,
                            <a href="http://joelgrus.com/" target="_blank">Joel Grus</a>,
                            <a href="http://markneumann.xyz/" target="_blank">Mark Neumann</a>,
                            <a href="https://allenai.org/team/oyvindt/" target="_blank">Oyvind Tafjord</a>,
                            <a href="http://www.cs.cmu.edu/~pdasigi/" target="_blank">Pradeep Dasigi</a>,
                            <b>Nelson F. Liu</b>,
                            <a href="http://matt-peters.github.io/" target="_blank">Matthew Peters</a>,
                            <a href="https://schmitztech.com/" target="_blank">Michael Schmitz</a>,
                            and <a href="https://www.cs.washington.edu/people/faculty/lsz/" target="_blank">Luke Zettlemoyer</a>.
                            <br/>
                            In <a href="https://nlposs.github.io/" target="_blank">
                                <b>ACL Workshop for Natural Language Processing Open Source Software (NLP-OSS)</b></a>, 2018.
                            <br/>
                            [<a href="papers/allennlp.nlposs2018.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#allennlp_nlposs2018_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/allenai/allennlp" target="_blank">code</a>]
                            <div id="allennlp_nlposs2018_abstract" class="abstract" style="display:none;">
                                <p>
                                    This paper describes AllenNLP, a platform
                                    for research on deep learning methods in
                                    natural language understanding. AllenNLP is
                                    designed to support researchers who want to
                                    build novel language understanding models
                                    quickly and easily. It is built on top of
                                    PyTorch, allowing for dynamic computation
                                    graphs, and provides (1) a flexible data API
                                    that handles intelligent batching and
                                    padding, (2) high-level abstractions for
                                    common operations in working with text, and
                                    (3) a modular and extensible experiment
                                    framework that makes doing good science
                                    easy. It also includes reference
                                    implementations of high quality approaches
                                    for both core semantic problems (e.g.
                                    semantic role labeling (Palmer et al.,
                                    2005)) and language understanding
                                    applications (e.g. machine comprehension
                                    (Rajpurkar et al., 2016)). AllenNLP is an
                                    ongoing open-source effort maintained by
                                    engineers and researchers at the Allen
                                    Institute for Artificial Intelligence.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/liu+levow+smith.sclem2018.pdf" target="_blank">
                                <b>Discovering Phonesthemes with Sparse Regularization</b>
                            </a>
                            <br/>
                            <b>Nelson F. Liu</b>,
                            <a href="https://faculty.washington.edu/levow/" target="_blank">Gina-Anne Levow</a>,
                            and <a href="https://homes.cs.washington.edu/~nasmith/" target="_blank">Noah A. Smith</a>.
                            <br/>
                            In <a href="https://sites.google.com/view/sclem2018/" target="_blank">
                                <b>NAACL Workshop on Subword and Character Level Models in NLP (SCLeM)</b></a>, 2018.
                            <br/>
                            [<a href="papers/liu+levow+smith.sclem2018.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#liu_levow_smith_sclem2018_abstract').toggle();return false;">abstract</a>]
                            [<a href="papers/liu+levow+smith.sclem2018.poster.pdf" target="_blank">poster</a>]
                            [<a href="papers/phonesthemes" target="_blank">code</a>]
                            <div id="liu_levow_smith_sclem2018_abstract" class="abstract" style="display:none;">
                                <p>
                                    We introduce a simple method for extracting
                                    non-arbitrary form-meaning representations
                                    from a collection of semantic vectors. We
                                    treat the problem as one of feature
                                    selection for a model trained to predict
                                    word vectors from subword features. We apply
                                    this model to the problem of automatically
                                    discovering phonesthemes, which are
                                    submorphemic sound clusters that appear in
                                    words with similar meaning. Many of our
                                    model-predicted phonesthemes overlap with
                                    those proposed in the linguistics
                                    literature, and we validate our approach
                                    with human judgments.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2017</h3>
                    <ul class="pl">
                        <li>
                            <a href="papers/welbl+liu+gardner.wnut2017.pdf" target="_blank">
                                <b>Crowdsourcing Multiple Choice Science Questions</b>
                            </a>
                            <br/>
                            <a href="https://jowel.gitlab.io/welbl/" target="_blank">Johannes Welbl</a>,
                            <b>Nelson F. Liu</b>,
                            and <a href="https://matt-gardner.github.io/" target="_blank">Matt Gardner</a>.
                            <br/>
                            In <a href="http://noisy-text.github.io/2017" target="_blank">
                                <b>EMNLP Workshop on Noisy User-generated Text</b></a>, 2017.
                            <br/>
                            [<a href="papers/welbl+liu+gardner.wnut2017.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#welbl_liu_gardner_wnut2017_abstract').toggle();return false;">abstract</a>]
                            [<a href="http://data.allenai.org/sciq/" target="_blank">data</a>]
                            [<a href="papers/sciq/welbl+liu+gardner.wnut2017.poster.pdf" target="_blank">poster</a>]
                            <div id="welbl_liu_gardner_wnut2017_abstract" class="abstract" style="display:none;">
                                <p>
                                    We present a novel method for obtaining
                                    high-quality, domain-targeted multiple
                                    choice questions from crowd workers.
                                    Generating these questions can be difficult
                                    without trading away originality, relevance
                                    or diversity in the answer options. Our
                                    method addresses these problems by
                                    leveraging a large corpus of domain-specific
                                    text and a small set of existing questions.
                                    It produces model suggestions for document
                                    selection and answer distractor choice which
                                    aid the human question generation process.
                                    With this method we have assembled SciQ, a
                                    dataset of 13.7K multiple choice science
                                    exam questions (Dataset available
                                    at <a href="http://data.allenai.org/sciq/"
                                    target="_blank">data.allenai.org/sciq/</a>).
                                    We demonstrate that the method produces
                                    in-domain questions by providing an analysis
                                    of this new dataset and by showing that
                                    humans cannot distinguish the crowdsourced
                                    questions from original questions. When
                                    using SciQ as additional training data to
                                    existing questions, we observe accuracy
                                    improvements on real science exams.
                                </p>
                            </div>
                        </li>
                    </ul> -->
                </div>
            </div>
    <!--        <hr>
            <div class="row">
                <div class="col">
                    <h2>Miscellany</h2>
                    <ul>
                        <li>
                            In my free time, I like to cycle (slowly) and climb
                            (indoors, poorly).
                        </li>
                        <li>
                            Feel free to
                            use <a href="https://github.com/nelson-liu/website"
                            target="_blank">this website's source code</a>, I'd
                            just appreciate if you linked back
                            to this page (<a href="https://nelsonliu.me/"
                            target="_blank">https://nelsonliu.me</a>).
                        </li>
                    </ul>
                </div>
            </div>
-->

            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <!-- <div class="col-6 col-md text-left align-self-center">
                       <p class="h5 text-muted">
                            © Nelson Liu, 2020
                        </p> 
                    </div> -->
                    <div class="col-6 col-md text-right">
                     <!--   <a href="https://nlp.stanford.edu" class="image-link">
                            <img class="mr-4" src="img/stanford_nlp_logo.gif" alt="Stanford NLP Group logo." height="75">
                        </a>
                       -->
<!--                         <a href="https://www.iitb.ac.in/" class="image-link">
                            <img src="img/iitb_logo.png" alt="IIT Bombay." height="75">
                        </a> -->
                    </div>
                </div>
            </footer>
        </div>
    </body>
</html>
